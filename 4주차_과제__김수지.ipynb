{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "4주차 과제_ 김수지",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## - 형태소 분석 \n",
        "\n",
        "> 파일 내용 및 출처 : 김영하 작가의 '살인자의 기억법' 교보문고 사이트 내 리뷰\n"
      ],
      "metadata": {
        "id": "2oA1tEkdlGtd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install konlpy"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ArxyQiCVUzOU",
        "outputId": "8723d9ae-1d99-402b-d7da-3e080928479b"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting konlpy\n",
            "  Downloading konlpy-0.6.0-py2.py3-none-any.whl (19.4 MB)\n",
            "\u001b[K     |████████████████████████████████| 19.4 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.6 in /usr/local/lib/python3.7/dist-packages (from konlpy) (1.21.5)\n",
            "Requirement already satisfied: lxml>=4.1.0 in /usr/local/lib/python3.7/dist-packages (from konlpy) (4.2.6)\n",
            "Collecting JPype1>=0.7.0\n",
            "  Downloading JPype1-1.3.0-cp37-cp37m-manylinux_2_5_x86_64.manylinux1_x86_64.whl (448 kB)\n",
            "\u001b[K     |████████████████████████████████| 448 kB 56.7 MB/s \n",
            "\u001b[?25hRequirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from JPype1>=0.7.0->konlpy) (3.10.0.2)\n",
            "Installing collected packages: JPype1, konlpy\n",
            "Successfully installed JPype1-1.3.0 konlpy-0.6.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 파일 내 일부 문장 형태소 분석\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "_-a6s5q9np3i"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import re\n",
        "from konlpy.tag import Komoran\n",
        "\n",
        "    \n",
        "def split_sentences(text):\n",
        "    all_sentences = []\n",
        "    lines = [line for line in text.strip().splitlines() if line.strip()]\n",
        "    \n",
        "    for line in lines:\n",
        "        sentences = re.split(\"(?<=[.?!])\\s+\", line) # 정규표현식\n",
        "        all_sentences += sentences\n",
        "    return all_sentences\n",
        "\n",
        "def main():\n",
        "    my_text = \"\"\"\n",
        "    과거 기억을 상실하면 내가 누구인지를 알 수 없게 되고 미래 기억을 못 하면 나는 영원히 현재에만 머무르게 된다. 과거와 미래가 없다면 현재는 무슨 의미일까.\n",
        "    우리는 죽음에 대한 근심으로 삶을 엉망으로 만들고 삶에 대한 걱정 때문에 죽음을 망쳐버린다.\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    komoran = Komoran()\n",
        "    results = []\n",
        "    sentences = split_sentences(my_text)\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        print(sentence)\n",
        "        \n",
        "    for sentence in sentences:\n",
        "        part = komoran.pos(sentence)    \n",
        "        results.append(part)             \n",
        "        \n",
        "    print(results)\n",
        "    print()\n",
        "\n",
        "    print(\"분절된 형태소만 출력\")\n",
        "    print(komoran.morphs(sentence))\n",
        "    print()\n",
        "\n",
        "    print(\"분절된 형태소 중 명사만 출력\")\n",
        "    print(komoran.nouns(sentence))\n",
        "\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "lNQqBYSzf4JU",
        "outputId": "123a64e6-dede-4ebf-c35f-f3a11a5916d6"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "과거 기억을 상실하면 내가 누구인지를 알 수 없게 되고 미래 기억을 못 하면 나는 영원히 현재에만 머무르게 된다.\n",
            "과거와 미래가 없다면 현재는 무슨 의미일까.\n",
            "    우리는 죽음에 대한 근심으로 삶을 엉망으로 만들고 삶에 대한 걱정 때문에 죽음을 망쳐버린다.\n",
            "[[('과거', 'NNG'), ('기억', 'NNG'), ('을', 'JKO'), ('상실', 'NNG'), ('하', 'XSV'), ('면', 'EC'), ('내', 'NP'), ('가', 'JKS'), ('누구', 'NP'), ('인지', 'NNG'), ('를', 'JKO'), ('알', 'VV'), ('ㄹ', 'ETM'), ('수', 'NNB'), ('없', 'VA'), ('게', 'EC'), ('되', 'VV'), ('고', 'EC'), ('미래', 'NNG'), ('기억', 'NNG'), ('을', 'JKO'), ('못', 'MAG'), ('하', 'VV'), ('면', 'EC'), ('나', 'NP'), ('는', 'JX'), ('영원히', 'MAG'), ('현재', 'NNG'), ('에', 'JKB'), ('만', 'JX'), ('머무르', 'VV'), ('게', 'EC'), ('되', 'VV'), ('ㄴ다', 'EF'), ('.', 'SF')], [('과거', 'NNG'), ('와', 'JC'), ('미래', 'NNG'), ('가', 'JKS'), ('없', 'VA'), ('다면', 'EC'), ('현재', 'NNG'), ('는', 'JX'), ('무슨', 'MM'), ('의미', 'NNG'), ('이', 'VCP'), ('ㄹ까', 'EF'), ('.', 'SF')], [('우리', 'NP'), ('는', 'JX'), ('죽음', 'NNG'), ('에', 'JKB'), ('대하', 'VV'), ('ㄴ', 'ETM'), ('근심', 'NNG'), ('으로', 'JKB'), ('삶', 'NNG'), ('을', 'JKO'), ('엉망', 'NNG'), ('으로', 'JKB'), ('만들', 'VV'), ('고', 'EC'), ('삶', 'NNG'), ('에', 'JKB'), ('대하', 'VV'), ('ㄴ', 'ETM'), ('걱정', 'NNG'), ('때문', 'NNB'), ('에', 'JKB'), ('죽음', 'NNG'), ('을', 'JKO'), ('망치', 'VV'), ('어', 'EC'), ('버리', 'VX'), ('ㄴ다', 'EF'), ('.', 'SF')]]\n",
            "\n",
            "분절된 형태소만 출력\n",
            "['우리', '는', '죽음', '에', '대하', 'ㄴ', '근심', '으로', '삶', '을', '엉망', '으로', '만들', '고', '삶', '에', '대하', 'ㄴ', '걱정', '때문', '에', '죽음', '을', '망치', '어', '버리', 'ㄴ다', '.']\n",
            "\n",
            "분절된 형태소 중 명사만 출력\n",
            "['죽음', '근심', '삶', '엉망', '삶', '걱정', '때문', '죽음']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "- 파일 분석"
      ],
      "metadata": {
        "id": "6t90kXRenkRy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "sIB8T_4rRGyo",
        "outputId": "0de408b7-db1b-4de3-c747-23feb37bb602"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "책: 7\n",
            "시간: 6\n",
            "소설: 6\n",
            "악: 5\n",
            "국내: 5\n",
            "작품: 5\n",
            "기억: 4\n",
            "내용: 4\n",
            "사람: 3\n",
            "속: 3\n"
          ]
        }
      ],
      "source": [
        "#명사 most common\n",
        "\n",
        "def split_sentences(text):    # 하나의 문서를 여러문장으로 나누기\n",
        "    import re    \n",
        "    all_sentences = []    \n",
        "    lines = [line for line in text.strip().splitlines() if line.strip()]\n",
        "    \n",
        "    for line in lines:\n",
        "        sentences = re.split(\"(?<=[.?!])\\s+\", line)\n",
        "        all_sentences += sentences\n",
        "    \n",
        "    return all_sentences\n",
        "\n",
        "\n",
        "\n",
        "def get_sentences(input_file_name):  #파일 불러오기\n",
        "    \n",
        "    sentences = []\n",
        "    \n",
        "    with open(input_file_name, \"r\", encoding=\"utf-8\") as input_file:\n",
        "        for sentence in input_file:\n",
        "            sub_sentences = split_sentences(sentence)  # 호출\n",
        "            sentences += sub_sentences\n",
        "    \n",
        "    return sentences\n",
        "\n",
        "\n",
        "\n",
        "def get_parts(sentences):   #코로란 형태소 분석기 실행\n",
        "    \n",
        "    from konlpy.tag import Komoran\n",
        "    \n",
        "    komoran = Komoran()\n",
        "    results = []\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        parts = komoran.pos(sentence)\n",
        "        for part in parts:\n",
        "            results.append(part)\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def filtered_parts(parts): # 주요품사만 추출\n",
        "    \n",
        "    N_POS = [\"NNG\", \"NNP\"]  # NNG : 일반명사, NNP : 고유명사\n",
        "    V_POS = [\"VV\", \"VA\"]    # \"VV\" : 동사, \"VA\" : 형용사\n",
        "    X_POS = [\"XR\"]          # \"XR\" : 어근\n",
        "    M_POS = [\"MAG\", \"MAJ\"]   # \"MAG\": 일반부사, \"MAJ\" : 접속부사\n",
        "    \n",
        "    filtered = []\n",
        "    \n",
        "    for part in parts:\n",
        "        if part[1] in N_POS:\n",
        "            filtered.append(part[0])\n",
        "        # elif part[1] in V_POS:\n",
        "        #    filtered.append(part[0]+\"다\") # 다가 필요한 경우\n",
        "        # elif part[1] in X_POS:\n",
        "        #    filtered.append(part[0]+\"하다\") # 하다 가 필요한 경우\n",
        "        # elif part[1] in M_POS:\n",
        "        #    filtered.append(part[0])\n",
        "        # else:\n",
        "        #    continue                \n",
        "    return filtered\n",
        "\n",
        "\n",
        "def word_count(filtered):    #빈도 체크하고 정렬\n",
        "    from collections import Counter    \n",
        "    wordform_counter = Counter()\n",
        "\n",
        "    for word in filtered:\n",
        "        wordform_counter[word] += 1    \n",
        "   # most_common() 순서대로 보여줌\n",
        "    for word, count in wordform_counter.most_common(10):  # most_common(10) 10개 추출\n",
        "        print(\"{}: {}\".format(word, count))    \n",
        "    \n",
        "def main():\n",
        "    input_file_name = \"book.txt\"\n",
        "    sentences = get_sentences(input_file_name)     \n",
        "    parts = get_parts(sentences)     \n",
        "    filtered = filtered_parts(parts)     \n",
        "    word_count(filtered)   \n",
        "  \n",
        "\n",
        "\n",
        "main()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 분석 결과는 파일 내 명사 중 가장 많이 언급된 10개를 보여준다. 책의 후기인만큼 '책'이 7번 나왔으며, '소설' 6번, '작품' 5번이 언급되었다. \n",
        "\n",
        "또한 '시간'이 6번, '악'이 5번, '기억'이 4번 언급된 것으로 볼 때, 이 책의 내용은 시간과 기억 그리고 악에 관한 것으로 유추할 수 있다. "
      ],
      "metadata": {
        "id": "nJ8GQZJmu0T3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#명사 형용사 부사 동사 most common\n",
        "\n",
        "def split_sentences(text):    # 하나의 문서를 여러문장으로 나누기\n",
        "    import re    \n",
        "    all_sentences = []    \n",
        "    lines = [line for line in text.strip().splitlines() if line.strip()]\n",
        "    \n",
        "    for line in lines:\n",
        "        sentences = re.split(\"(?<=[.?!])\\s+\", line)\n",
        "        all_sentences += sentences\n",
        "    \n",
        "    return all_sentences\n",
        "\n",
        "\n",
        "\n",
        "def get_sentences(input_file_name):  #파일 불러오기\n",
        "    \n",
        "    sentences = []\n",
        "    \n",
        "    with open(input_file_name, \"r\", encoding=\"utf-8\") as input_file:\n",
        "        for sentence in input_file:\n",
        "            sub_sentences = split_sentences(sentence)  # 호출\n",
        "            sentences += sub_sentences\n",
        "    \n",
        "    return sentences\n",
        "\n",
        "\n",
        "\n",
        "def get_parts(sentences):   #코로란 형태소 분석기 실행\n",
        "    \n",
        "    from konlpy.tag import Komoran\n",
        "    \n",
        "    komoran = Komoran()\n",
        "    results = []\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        parts = komoran.pos(sentence)\n",
        "        for part in parts:\n",
        "            results.append(part)\n",
        "    \n",
        "    return results\n",
        "\n",
        "\n",
        "def filtered_parts(parts): # 주요품사만 추출\n",
        "    \n",
        "    N_POS = [\"NNG\", \"NNP\"]  # NNG : 일반명사, NNP : 고유명사\n",
        "    V_POS = [\"VV\", \"VA\"]    # \"VV\" : 동사, \"VA\" : 형용사\n",
        "    X_POS = [\"XR\"]          # \"XR\" : 어근\n",
        "    M_POS = [\"MAG\", \"MAJ\"]   # \"MAG\": 일반부사, \"MAJ\" : 접속부사\n",
        "    \n",
        "    filtered = []\n",
        "    \n",
        "    for part in parts:\n",
        "        if part[1] in N_POS:\n",
        "            filtered.append(part[0])\n",
        "        elif part[1] in V_POS:\n",
        "           filtered.append(part[0]+\"다\") # 다가 필요한 경우\n",
        "        elif part[1] in X_POS:\n",
        "           filtered.append(part[0]+\"하다\") # 하다 가 필요한 경우\n",
        "        elif part[1] in M_POS:\n",
        "           filtered.append(part[0])\n",
        "        else:\n",
        "           continue                \n",
        "    return filtered\n",
        "\n",
        "\n",
        "def word_count(filtered):    #빈도 체크하고 정렬\n",
        "    from collections import Counter    \n",
        "    wordform_counter = Counter()\n",
        "\n",
        "    for word in filtered:\n",
        "        wordform_counter[word] += 1    \n",
        "   # most_common() 순서대로 보여줌\n",
        "    for word, count in wordform_counter.most_common(30):  # most_common(30) 30개 추출\n",
        "        print(\"{}: {}\".format(word, count))    \n",
        "    \n",
        "def main():\n",
        "    input_file_name = \"book.txt\"\n",
        "    sentences = get_sentences(input_file_name)     \n",
        "    parts = get_parts(sentences)     \n",
        "    filtered = filtered_parts(parts)     \n",
        "    word_count(filtered)   \n",
        "  \n",
        "\n",
        "\n",
        "main()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Ob8J4K3CuzIW",
        "outputId": "cd762677-994c-4c3e-b236-bd84e8a6deac"
      },
      "execution_count": 33,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "없다: 11\n",
            "읽다: 10\n",
            "되다: 8\n",
            "책: 7\n",
            "시간: 6\n",
            "소설: 6\n",
            "하다: 5\n",
            "있다: 5\n",
            "악: 5\n",
            "좋다: 5\n",
            "국내: 5\n",
            "작품: 5\n",
            "기억: 4\n",
            "보다: 4\n",
            "내용: 4\n",
            "알다: 3\n",
            "느끼다: 3\n",
            "사람: 3\n",
            "속: 3\n",
            "읽히다: 3\n",
            "하지만: 3\n",
            "처음: 3\n",
            "이해: 3\n",
            "과거: 2\n",
            "인지: 2\n",
            "미래: 2\n",
            "현재: 2\n",
            "죽음: 2\n",
            "대하다: 2\n",
            "삶: 2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "이번에는 명사 외에도 동사, 형용사, 부사를 포함하여 문장 분석을 한 결과이다. \n",
        "'없다'라는 동사가 11번으로 가장 많이 언급되었으며 이는 부정적인 표현으로 어느정도 책의 내용이 어둡다고 유추할 수 있다. \n",
        "\n",
        "'읽다', '책', '소설', '작품' 등이 각각 10, 7, 6, 5번씩 언급된 것은 파일이 책의 리뷰에 관한 것이기 때문으로 생각할 수 있다.\n",
        "\n",
        "'악', '기억', '과거', '미래', '현재', '죽음', '삶'이라는 명사들이 언급된 결과로 볼 때 책의 내용은 삶의 시간들과 죽음, 악, 그리고 기억에 관할 것으로 예상된다. "
      ],
      "metadata": {
        "id": "xTKMgeCnvvpT"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "외국어, 분석불능범주 처리"
      ],
      "metadata": {
        "id": "lZoLxwt7l0w8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import re\n",
        "from konlpy.tag import Komoran\n",
        "\n",
        "    \n",
        "def split_sentences(text):\n",
        "    all_sentences = []\n",
        "    lines = [line for line in text.strip().splitlines() if line.strip()]\n",
        "    \n",
        "    for line in lines:\n",
        "        sentences = re.split(\"(?<=[.?!])\\s+\", line) # 정규표현식\n",
        "        all_sentences += sentences\n",
        "    return all_sentences\n",
        "\n",
        "def main():\n",
        "    my_text = \"\"\"\n",
        "    종이책으로 출간된 지가 벌써 5년이라는 시간이 훌쩍 넘어버린 조금은 오래된 국내 소설이었다. 주로 일본소설과 코믹스, 국내 작품 중에서는 주로 로맨스 계열을 보다 오랜만에 국내 소설을 보게 되니 -그것도 우연찮은 기회에 이 작품을 원작으로 한 영화가 개봉되었다는 정보를 얻게 되어 보게 되었지만-뭔가 미묘한 느낌이 들었다고 해야 할까. 국내 소설도 재미있는 건 재밌다는 것을 새삼 느낄 수 있었다. 스토리 측면에서는 이미 다른 분들이 스포일러가 다분히 포함된 글을 올렸고, 네이버의 도움을 받으면 이 작품의 전반적인 줄거리는 알 수 있을 것이다. 하지만 여기서는 느낄 수 없는 분위기는 책을 읽지 않은 분들은 이해하기 힘들지 않을까 싶었다.\n",
        "    속도에 속지 말라는 서평이 아주 와닿는다. 알츠하이머에 걸린 노인의 시간에 맞춰 전개되는 순간순간이 놀랍도록 섬뜩하다\n",
        "\n",
        "\n",
        "\"\"\"\n",
        "    komoran = Komoran()\n",
        "    results = []\n",
        "    sentences = split_sentences(my_text)\n",
        "    \n",
        "    for sentence in sentences:\n",
        "        print(sentence)\n",
        "        \n",
        "    for sentence in sentences:\n",
        "        part = komoran.pos(sentence)    \n",
        "        results.append(part)             \n",
        "        \n",
        "    print(results)\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YFPTQSSnUdUq",
        "outputId": "775402bb-94b1-4637-e073-6aa4d6237f91"
      },
      "execution_count": 24,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "종이책으로 출간된 지가 벌써 5년이라는 시간이 훌쩍 넘어버린 조금은 오래된 국내 소설이었다.\n",
            "주로 일본소설과 코믹스, 국내 작품 중에서는 주로 로맨스 계열을 보다 오랜만에 국내 소설을 보게 되니 -그것도 우연찮은 기회에 이 작품을 원작으로 한 영화가 개봉되었다는 정보를 얻게 되어 보게 되었지만-뭔가 미묘한 느낌이 들었다고 해야 할까.\n",
            "국내 소설도 재미있는 건 재밌다는 것을 새삼 느낄 수 있었다.\n",
            "스토리 측면에서는 이미 다른 분들이 스포일러가 다분히 포함된 글을 올렸고, 네이버의 도움을 받으면 이 작품의 전반적인 줄거리는 알 수 있을 것이다.\n",
            "하지만 여기서는 느낄 수 없는 분위기는 책을 읽지 않은 분들은 이해하기 힘들지 않을까 싶었다.\n",
            "    속도에 속지 말라는 서평이 아주 와닿는다.\n",
            "알츠하이머에 걸린 노인의 시간에 맞춰 전개되는 순간순간이 놀랍도록 섬뜩하다\n",
            "[[('종이', 'NNG'), ('책', 'NNG'), ('으로', 'JKB'), ('출간', 'NNG'), ('되', 'XSV'), ('ㄴ', 'ETM'), ('지', 'NNB'), ('가', 'JKS'), ('벌써', 'MAG'), ('5', 'SN'), ('년', 'NNB'), ('이', 'VCP'), ('라는', 'ETM'), ('시간', 'NNG'), ('이', 'JKS'), ('훌쩍', 'MAG'), ('넘', 'VV'), ('어', 'EC'), ('버리', 'VX'), ('ㄴ', 'ETM'), ('조금', 'NNG'), ('은', 'JX'), ('오래되', 'VV'), ('ㄴ', 'ETM'), ('국내', 'NNG'), ('소설', 'NNG'), ('이', 'VCP'), ('었', 'EP'), ('다', 'EF'), ('.', 'SF')], [('주로', 'MAG'), ('일본', 'NNP'), ('소설', 'NNP'), ('과', 'JC'), ('코', 'NNG'), ('믹스', 'NNP'), (',', 'SP'), ('국내', 'NNG'), ('작품', 'NNG'), ('중', 'NNB'), ('에서', 'JKB'), ('는', 'JX'), ('주로', 'MAG'), ('로맨스', 'NNP'), ('계열', 'NNG'), ('을', 'JKO'), ('보다', 'MAG'), ('오랜만', 'NNG'), ('에', 'JKB'), ('국내', 'NNG'), ('소설', 'NNG'), ('을', 'JKO'), ('보', 'VV'), ('게', 'EC'), ('되', 'VV'), ('니', 'EC'), ('-', 'SS'), ('그것', 'NP'), ('도', 'JX'), ('우연찮', 'VA'), ('은', 'ETM'), ('기회', 'NNG'), ('에', 'JKB'), ('이', 'MM'), ('작품', 'NNG'), ('을', 'JKO'), ('원작', 'NNP'), ('으로', 'JKB'), ('한', 'MM'), ('영화', 'NNG'), ('가', 'JKS'), ('개봉', 'NNG'), ('되', 'XSV'), ('었', 'EP'), ('다는', 'ETM'), ('정보', 'NNG'), ('를', 'JKO'), ('얻', 'VV'), ('게', 'EC'), ('되', 'VV'), ('어', 'EC'), ('보', 'VV'), ('게', 'EC'), ('되', 'VV'), ('었', 'EP'), ('지만', 'EC'), ('-', 'SS'), ('뭐', 'NP'), ('ㄴ가', 'EC'), ('미묘', 'XR'), ('하', 'XSA'), ('ㄴ', 'ETM'), ('느낌', 'NNG'), ('이', 'JKS'), ('들', 'VV'), ('었', 'EP'), ('다고', 'EC'), ('하', 'VV'), ('아야', 'EC'), ('하', 'VX'), ('ㄹ까', 'EF'), ('.', 'SF')], [('국내', 'NNG'), ('소설', 'NNG'), ('도', 'JX'), ('재미있', 'VA'), ('는', 'ETM'), ('건', 'NNB'), ('재밌', 'VA'), ('다는', 'ETM'), ('것', 'NNB'), ('을', 'JKO'), ('새삼', 'MAG'), ('느끼', 'VV'), ('ㄹ', 'ETM'), ('수', 'NNB'), ('있', 'VX'), ('었', 'EP'), ('다', 'EF'), ('.', 'SF')], [('스토리', 'NNP'), ('측면', 'NNG'), ('에서', 'JKB'), ('는', 'JX'), ('이미', 'MAG'), ('다른', 'MM'), ('분', 'NNB'), ('들', 'XSN'), ('이', 'JKS'), ('스포일러', 'NNP'), ('가', 'JKS'), ('다분히', 'MAG'), ('포함', 'NNG'), ('되', 'XSV'), ('ㄴ', 'ETM'), ('글', 'NNG'), ('을', 'JKO'), ('올리', 'VV'), ('었', 'EP'), ('고', 'EC'), (',', 'SP'), ('네이버', 'NNP'), ('의', 'JKG'), ('도움', 'NNG'), ('을', 'JKO'), ('받', 'VV'), ('으면', 'EC'), ('이', 'MM'), ('작품', 'NNG'), ('의', 'JKG'), ('전반', 'NNG'), ('적', 'XSN'), ('이', 'VCP'), ('ㄴ', 'ETM'), ('줄거리', 'NNG'), ('는', 'JX'), ('알', 'VV'), ('ㄹ', 'ETM'), ('수', 'NNB'), ('있', 'VV'), ('을', 'ETM'), ('것', 'NNB'), ('이', 'VCP'), ('다', 'EF'), ('.', 'SF')], [('하지만', 'MAJ'), ('여기', 'NP'), ('서', 'JKB'), ('는', 'JX'), ('느끼', 'VV'), ('ㄹ', 'ETM'), ('수', 'NNB'), ('없', 'VA'), ('는', 'ETM'), ('분위기', 'NNG'), ('는', 'JX'), ('책', 'NNG'), ('을', 'JKO'), ('읽', 'VV'), ('지', 'EC'), ('않', 'VX'), ('은', 'ETM'), ('분', 'NNB'), ('들', 'XSN'), ('은', 'JX'), ('이해', 'NNG'), ('하', 'XSV'), ('기', 'ETN'), ('힘들', 'VA'), ('지', 'EC'), ('않', 'VX'), ('을까', 'EC'), ('싶', 'VX'), ('었', 'EP'), ('다', 'EF'), ('.', 'SF')], [('속도', 'NNG'), ('에', 'JKB'), ('속', 'VV'), ('지', 'EC'), ('말', 'VX'), ('라는', 'ETM'), ('서평', 'NNP'), ('이', 'JKS'), ('아주', 'MAG'), ('오', 'VV'), ('아', 'EC'), ('닿', 'VV'), ('는다', 'EF'), ('.', 'SF')], [('알츠하이머에', 'NA'), ('걸리', 'VV'), ('ㄴ', 'ETM'), ('노인', 'NNG'), ('의', 'JKG'), ('시간', 'NNG'), ('에', 'JKB'), ('맞추', 'VV'), ('어', 'EC'), ('전개', 'NNG'), ('되', 'XSV'), ('는', 'ETM'), ('순간순간', 'NNG'), ('이', 'JKS'), ('놀랍', 'VA'), ('도록', 'EC'), ('섬뜩', 'MAG'), ('하', 'XSV'), ('다', 'EC')]]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "위 결과 스포일러, 네이버, 로맨스, 스토리는 NNP (고유 명사)로 처리가 되었지만 알츠하이머는 (알츠하이머에, NA),  코믹스는 (코, NNG), (믹스, NNP)으로 처리가 된 것을 확인할 수 있다.\n"
      ],
      "metadata": {
        "id": "ioMm5vjNmq6l"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 코믹스 \n",
        "from konlpy.tag import Kkma, Hannanum, Komoran, Mecab, Twitter\n",
        "\n",
        "Twitter = Twitter()\n",
        "kkma = Kkma()\n",
        "hannanum = Hannanum()\n",
        "komoran = Komoran()\n",
        "text=\"주로 일본소설과 코믹스, 국내 작품 중에서는 주로 로맨스 계열을 보다 오랜만에 국내 소설을 보게 되니 -그것도 우연찮은 기회에 이 작품을 원작으로 한 영화가 개봉되었다는 정보를 얻게 되어 보게 되었지만\"\n",
        "\n",
        "#komoran\n",
        "print('komoran', komoran.pos(text))\n",
        "print()\n",
        "# Hannanum\n",
        "print('Hannanum', hannanum.pos(text))\n",
        "print()\n",
        "# Kkma\n",
        "print('Kkma', kkma.pos(text))\n",
        "print()\n",
        "#Twitter\n",
        "print('Twitter', Twitter.pos(text))\n",
        "print()\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ANpzykf5nfFz",
        "outputId": "17ee0ceb-924c-4e9d-d339-7b64c709bcfb"
      },
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/konlpy/tag/_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
            "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hannanum [('주로', 'M'), ('일본소설', 'N'), ('과', 'J'), ('코믹스,', 'N'), ('국내', 'N'), ('작품', 'N'), ('중', 'N'), ('에서는', 'J'), ('주로', 'M'), ('로맨스', 'N'), ('계열', 'N'), ('을', 'J'), ('보다', 'M'), ('오랜만', 'N'), ('에', 'J'), ('국내', 'N'), ('소설', 'N'), ('을', 'J'), ('보', 'P'), ('게', 'E'), ('되', 'P'), ('니', 'E'), ('-', 'S'), ('그것', 'N'), ('도', 'J'), ('우연찮은', 'N'), ('기회', 'N'), ('에', 'J'), ('이', 'M'), ('작품', 'N'), ('을', 'J'), ('원작', 'N'), ('으로', 'J'), ('하', 'P'), ('ㄴ', 'E'), ('영화', 'N'), ('가', 'J'), ('개봉', 'N'), ('되', 'X'), ('었다는', 'E'), ('정보', 'N'), ('를', 'J'), ('얻', 'P'), ('게', 'E'), ('되', 'P'), ('어', 'E'), ('보', 'P'), ('게', 'E'), ('되', 'N'), ('이', 'J'), ('었지만', 'E')]\n",
            "\n",
            "Kkma [('주로', 'MAG'), ('일본', 'NNG'), ('소설', 'NNG'), ('과', 'JKM'), ('코믹스', 'NNG'), (',', 'SP'), ('국내', 'NNG'), ('작품', 'NNG'), ('중', 'NNB'), ('에서', 'JKM'), ('는', 'JX'), ('주로', 'MAG'), ('로맨스', 'NNG'), ('계열', 'NNG'), ('을', 'JKO'), ('보다', 'MAG'), ('오랜만', 'NNG'), ('에', 'JKM'), ('국내', 'NNG'), ('소설', 'NNG'), ('을', 'JKO'), ('보', 'VV'), ('게', 'ECD'), ('되', 'VV'), ('니', 'ECD'), ('-', 'SW'), ('그것', 'NP'), ('도', 'JX'), ('우연찮', 'VA'), ('은', 'ETD'), ('기회', 'NNG'), ('에', 'JKM'), ('이', 'MDT'), ('작품', 'NNG'), ('을', 'JKO'), ('원작', 'NNG'), ('으로', 'JKM'), ('한', 'MDN'), ('영화', 'NNG'), ('가', 'JKS'), ('개봉', 'NNG'), ('되', 'XSV'), ('었', 'EPT'), ('다는', 'ETD'), ('정보', 'NNG'), ('를', 'JKO'), ('얻', 'VV'), ('게', 'ECD'), ('되', 'VV'), ('어', 'ECD'), ('보', 'VV'), ('게', 'ECD'), ('되', 'VV'), ('었', 'EPT'), ('지만', 'ECE')]\n",
            "\n",
            "komoran [('주로', 'MAG'), ('일본', 'NNP'), ('소설', 'NNP'), ('과', 'JC'), ('코', 'NNG'), ('믹스', 'NNP'), (',', 'SP'), ('국내', 'NNG'), ('작품', 'NNG'), ('중', 'NNB'), ('에서', 'JKB'), ('는', 'JX'), ('주로', 'MAG'), ('로맨스', 'NNP'), ('계열', 'NNG'), ('을', 'JKO'), ('보다', 'MAG'), ('오랜만', 'NNG'), ('에', 'JKB'), ('국내', 'NNG'), ('소설', 'NNG'), ('을', 'JKO'), ('보', 'VV'), ('게', 'EC'), ('되', 'VV'), ('니', 'EC'), ('-', 'SS'), ('그것', 'NP'), ('도', 'JX'), ('우연찮', 'VA'), ('은', 'ETM'), ('기회', 'NNG'), ('에', 'JKB'), ('이', 'MM'), ('작품', 'NNG'), ('을', 'JKO'), ('원작', 'NNP'), ('으로', 'JKB'), ('한', 'MM'), ('영화', 'NNG'), ('가', 'JKS'), ('개봉', 'NNG'), ('되', 'XSV'), ('었', 'EP'), ('다는', 'ETM'), ('정보', 'NNG'), ('를', 'JKO'), ('얻', 'VV'), ('게', 'EC'), ('되', 'VV'), ('어', 'EC'), ('보', 'VV'), ('게', 'EC'), ('되', 'VV'), ('었', 'EP'), ('지만', 'EC')]\n",
            "\n",
            "Twitter [('주로', 'Noun'), ('일', 'Modifier'), ('본', 'Modifier'), ('소설', 'Noun'), ('과', 'Josa'), ('코믹스', 'Noun'), (',', 'Punctuation'), ('국내', 'Noun'), ('작품', 'Noun'), ('중', 'Noun'), ('에서는', 'Josa'), ('주로', 'Noun'), ('로맨스', 'Noun'), ('계열', 'Noun'), ('을', 'Josa'), ('보다', 'Verb'), ('오랜', 'Modifier'), ('만', 'Noun'), ('에', 'Josa'), ('국내', 'Noun'), ('소설', 'Noun'), ('을', 'Josa'), ('보게', 'Verb'), ('되니', 'Verb'), ('-', 'Punctuation'), ('그것', 'Noun'), ('도', 'Josa'), ('우연찮', 'Noun'), ('은', 'Josa'), ('기회', 'Noun'), ('에', 'Josa'), ('이', 'Noun'), ('작품', 'Noun'), ('을', 'Josa'), ('원작', 'Noun'), ('으로', 'Josa'), ('한', 'Verb'), ('영화', 'Noun'), ('가', 'Josa'), ('개봉', 'Noun'), ('되었다는', 'Verb'), ('정보', 'Noun'), ('를', 'Josa'), ('얻게', 'Verb'), ('되어', 'Verb'), ('보게', 'Verb'), ('되었지만', 'Verb')]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "komoran으로 처리되지 못한 '코믹스'를 Hannanum, Kkma, Twitter로 처리한 결과 각각 N, NNG, Noun  으로 처리된 것을 확인할 수 있다.  "
      ],
      "metadata": {
        "id": "z5frIjUCq_zt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "#알츠하이머\n",
        "from konlpy.tag import Kkma, Hannanum, Komoran, Mecab, Twitter\n",
        "\n",
        "Twitter = Twitter()\n",
        "kkma = Kkma()\n",
        "hannanum = Hannanum()\n",
        "komoran = Komoran()\n",
        "text=\"알츠하이머에 걸린 노인의 시간에 맞춰 전개되는 순간순간이 놀랍도록 섬뜩하다\"\n",
        "\n",
        "#komoran\n",
        "print('komoran', komoran.pos(text))\n",
        "print()\n",
        "# Hannanum\n",
        "print('Hannanum', hannanum.pos(text))\n",
        "print()\n",
        "# Kkma\n",
        "print('Kkma', kkma.pos(text))\n",
        "print()\n",
        "#Twitter\n",
        "print('Twitter', Twitter.pos(text))\n",
        "print()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Pi7Zxwr2kTjO",
        "outputId": "a33fadbe-3e31-4541-f490-459ae4b03907"
      },
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/konlpy/tag/_okt.py:17: UserWarning: \"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.\n",
            "  warn('\"Twitter\" has changed to \"Okt\" since KoNLPy v0.4.5.')\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Hannanum [('알츠하이머', 'N'), ('에', 'J'), ('걸리', 'P'), ('ㄴ', 'E'), ('노인', 'N'), ('의', 'J'), ('시간', 'N'), ('에', 'J'), ('맞추', 'P'), ('어', 'E'), ('전개', 'N'), ('되', 'X'), ('는', 'E'), ('순간순간', 'N'), ('이', 'J'), ('놀랍', 'P'), ('도록', 'E'), ('섬뜩', 'N'), ('하', 'X'), ('다', 'E')]\n",
            "\n",
            "Kkma [('알츠하이머', 'NNG'), ('에', 'JKM'), ('걸리', 'VV'), ('ㄴ', 'ETD'), ('노인', 'NNG'), ('의', 'JKG'), ('시간', 'NNG'), ('에', 'JKM'), ('맞추', 'VV'), ('어', 'ECS'), ('전개', 'NNG'), ('되', 'XSV'), ('는', 'ETD'), ('순간순간', 'NNG'), ('이', 'JKS'), ('놀랍', 'VA'), ('도록', 'ECD'), ('섬뜩하', 'VA'), ('다', 'EFN')]\n",
            "\n",
            "komoran [('알츠하이머에', 'NA'), ('걸리', 'VV'), ('ㄴ', 'ETM'), ('노인', 'NNG'), ('의', 'JKG'), ('시간', 'NNG'), ('에', 'JKB'), ('맞추', 'VV'), ('어', 'EC'), ('전개', 'NNG'), ('되', 'XSV'), ('는', 'ETM'), ('순간순간', 'NNG'), ('이', 'JKS'), ('놀랍', 'VA'), ('도록', 'EC'), ('섬뜩', 'MAG'), ('하', 'XSV'), ('다', 'EC')]\n",
            "\n",
            "Twitter [('알츠하이머', 'Noun'), ('에', 'Josa'), ('걸린', 'Verb'), ('노인', 'Noun'), ('의', 'Josa'), ('시간', 'Noun'), ('에', 'Josa'), ('맞춰', 'Verb'), ('전개', 'Noun'), ('되는', 'Verb'), ('순간', 'Noun'), ('순간', 'Noun'), ('이', 'Josa'), ('놀랍도록', 'Verb'), ('섬뜩하다', 'Adjective')]\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "알츠하이머 역시 komoran으로 처리되지 못하였지만 Hannanum, Kkma, Twitter로 처리한 결과 각각 N, NNG, Noun  으로 처리된 것을 확인할 수 있다. "
      ],
      "metadata": {
        "id": "sSR51q4grm3Q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "결론 : \n",
        " \n",
        "\n",
        "> 위 분석에서 사용한 파일은 책의 후기를 담은 파일인만큼 '읽다', '책', '소설' 등이 많이 언급되었음을 분석을 통해 알 수 있었다. \n",
        "\n",
        "\n",
        "> 또한 자주 등장하는 명사들이 '악', '기억', '과거', '현재', '미래' 인 것으로 볼 때 책의 내용이 대략 이러한 주제를 담고 있다고 예측할 수 있다.\n",
        "\n",
        "\n",
        "> 이처럼 파일의 내용이 무엇인지 알 수 없어도 문장 분석을 통해 파일의 내용을 대략적으로 유추할 수 있으며, 이는 더욱 더 다양하게 활용될 수 있다.\n",
        "\n"
      ],
      "metadata": {
        "id": "Odgj65vnxrEh"
      }
    }
  ]
}